# Agente de IA com Ollama + LangChain

## Este projeto implementa um agente de IA local utilizando as seguintes tecnologias:

- Ollama (em container Docker)
- Python 3.13
- uv como ferramenta de build
- LangChain para orquestraÃ§Ã£o
- Modelo base: TinyLlama (configurÃ¡vel)
- Estrutura do Projeto


`   ia-agent/
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ logs_review.py           
    â”œâ”€â”€ docker-compose.yml        
    â”œâ”€â”€ pyproject.toml           
    â”œâ”€â”€ README.md                 
    â””â”€â”€ .gitignore `
            
### Requisitos
Antes de executar o projeto, certifique-se de ter instalado:

***Docker***
***Docker Compose***
***Python 3.13 ou superior***
***uv (pip install uv ou via Rust/Cargo)***
***Como Rodar o Projeto***



Objetivo

O agente tem como finalidade ler os arquivos de log localizados no diretÃ³rio:

`src/logs/`

A partir dos logs encontrados, o AI-Agent realiza uma anÃ¡lise detalhada e responde em linguagem natural o que estÃ¡ acontecendo na aplicaÃ§Ã£o. Ele interpreta erros, identifica comportamentos suspeitos ou inesperados, e resume os eventos importantes, auxiliando no monitoramento e depuraÃ§Ã£o do sistema.

âš™ï¸ Tecnologia

Este projeto foi construÃ­do com o framework LangChain, que permite criar agentes inteligentes capazes de interagir com documentos, APIs e fluxos de dados de forma contextual e conversacional.

ğŸš€ Como Executar

Para executar o projeto, Ã© necessÃ¡rio ter o Docker e o Docker Compose instalados. Em seguida, rode o seguinte comando no terminal, a partir da raiz do projeto:

`docker-compose up --build`

Esse comando irÃ¡:

Construir a imagem do container, caso ainda nÃ£o tenha sido construÃ­da;

Subir o container com o agente configurado;

Iniciar a anÃ¡lise dos arquivos de log presentes em src/logs/ automaticamente (dependendo da configuraÃ§Ã£o do container).

 Estrutura do Projeto

.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ src
â”‚   â””â”€â”€ model
â”‚       â””â”€â”€ chat.py
â”‚   â””â”€â”€ logs
â”‚       â””â”€â”€ election-management.log
â”‚   â””â”€â”€ test
â”‚       â””â”€â”€ chat_test.py
â”‚   â””â”€â”€ logs_review.py
â””â”€â”€ README.md

src/logs/: DiretÃ³rio onde os arquivos de log devem ser colocados.

`python /src/test/chat_test.py`: Script para testar a conexÃ£o com o ollama

`python /src/logs_review.py`: Script principal que contÃ©m a lÃ³gica de leitura e anÃ¡lise dos logs.


[!NOTE]
âš ï¸ Caso esteja utilizando uma imagem de modelo de IA local (como modelos LLM offline), o desempenho serÃ¡ significativamente melhor em sistemas com GPU instalada (NVIDIA, com drivers apropriados). Em ambientes com GPU, o tempo de resposta pode ser de 10 a 100 vezes mais rÃ¡pido.

Em sistemas sem GPU, a IA serÃ¡ executada utilizando a CPU, o que pode resultar em tempos de execuÃ§Ã£o muito mais lentos, especialmente ao processar grandes volumes de log ou modelos complexos.

Recomendado para ambientes Linux com GPU ou Windows com suporte Ã  aceleraÃ§Ã£o por hardware.
